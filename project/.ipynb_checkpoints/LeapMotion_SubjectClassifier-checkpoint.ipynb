{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import listdir\n",
    "from os.path import isfile, join, dirname, abspath\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length of records (if shorter, we need to add some zero rows)\n",
    "NUMBER_TIMESTEPS = 100\n",
    "# the number of features (from the data)\n",
    "NUMBER_FEATURES = 202\n",
    "# the number of classes/gestures\n",
    "NUMBER_OUTPUTS = 2\n",
    "# you can encode more than 1 but for this example we have binary output (circle/swipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory where your data is\n",
    "mypath = './data'\n",
    "\n",
    "# creating a list with all the filenames\n",
    "datafiles = [f for f in listdir('data') if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose data we need\n",
    "columns = ['handPalmPosition_X','handPalmPosition_Y','handPalmPosition_Z',\n",
    "          'pitch', 'roll', 'yaw', 'GestureTypeCircle', 'GestureTypeSwipe',\n",
    "          'wristPosition_X', 'wristPosition_Y','wristPosition_Z',\n",
    "          'elbowPosition_X', 'elbowPosition_Y', 'elbowPosition_Z']\n",
    "\n",
    "finger_names = ['Thumb', 'Index', 'Middle', 'Ring', 'Pinky']\n",
    "bone_names = ['Metacarpal', 'Proximal', 'Intermediate', 'Distal']\n",
    "    \n",
    "for finger in finger_names:\n",
    "    columns.append(finger + 'Length')\n",
    "    columns.append(finger + 'Width')\n",
    "\n",
    "for finger in finger_names:\n",
    "    for bone in bone_names:\n",
    "        columns.append(finger + bone + 'Start_X')\n",
    "        columns.append(finger + bone + 'Start_Y')\n",
    "        columns.append(finger + bone + 'Start_Z')\n",
    "        columns.append(finger + bone + 'End_X')\n",
    "        columns.append(finger + bone + 'End_Y')\n",
    "        columns.append(finger + bone + 'End_Z')\n",
    "        columns.append(finger + bone + 'Direction_X') \n",
    "        columns.append(finger + bone + 'Direction_Y') \n",
    "        columns.append(finger + bone + 'Direction_Z')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\circle1.csv\n",
      "size raw = (37, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle10.csv\n",
      "size raw = (60, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle2.csv\n",
      "size raw = (66, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle3.csv\n",
      "size raw = (39, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle4.csv\n",
      "size raw = (39, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle5.csv\n",
      "size raw = (78, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle6.csv\n",
      "size raw = (129, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle7.csv\n",
      "size raw = (80, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle8.csv\n",
      "size raw = (97, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\circle9.csv\n",
      "size raw = (67, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe1.csv\n",
      "size raw = (84, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe10.csv\n",
      "size raw = (84, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe2.csv\n",
      "size raw = (39, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe3.csv\n",
      "size raw = (62, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe4.csv\n",
      "size raw = (59, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe5.csv\n",
      "size raw = (47, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe6.csv\n",
      "size raw = (99, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe7.csv\n",
      "size raw = (92, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe8.csv\n",
      "size raw = (51, 204)\n",
      "size normalized =  (100, 204)\n",
      "data\\swipe9.csv\n",
      "size raw = (99, 204)\n",
      "size normalized =  (100, 204)\n"
     ]
    }
   ],
   "source": [
    "# Features\n",
    "x = []\n",
    "# Labels\n",
    "y = []\n",
    "\n",
    "for sample in datafiles:\n",
    "    relative_path = 'data\\\\' + sample\n",
    "    tmp = pd.read_csv(relative_path, usecols=columns)\n",
    "    \n",
    "    # Normalize the sample size: LSTM requires all inputs of the same size!\n",
    "    print('{}\\nsize raw = {}'.format(relative_path,tmp.shape))\n",
    "    while tmp.shape[0] < NUMBER_TIMESTEPS:\n",
    "        tmp = tmp.append(pd.Series(0, index=tmp.columns), ignore_index=True)\n",
    "\n",
    "    if tmp.shape[0] > NUMBER_TIMESTEPS:\n",
    "        tmp = tmp.head(100)\n",
    "    print('size normalized = ',tmp.shape)\n",
    "    \n",
    "    tmp_x = tmp[[column for column in list(tmp.columns)\n",
    "                 if column != 'GestureTypeCircle' \n",
    "                 and column != 'GestureTypeSwipe']]\n",
    "    \n",
    "    # subject 1 --> tmp_y = [1, 0]\n",
    "    tmp_y = [1, 0]\n",
    "    if sample.find('sub1') == -1:\n",
    "        # subject 2 --> tmp_y = [0, 1]\n",
    "        tmp_y = [1, 0]\n",
    "        \n",
    "    x.append(tmp_x)\n",
    "    y.append(tmp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y[0].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Each sample requires labels of [1,NUMBER_OUTPUTS] size (not a list)\n",
    "y_new = list()\n",
    "for cur_label in y:\n",
    "    tmp = np.array(cur_label.loc[0])\n",
    "    y_new.append(tmp)\n",
    "y = np.array(y_new)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 14\n",
      "Number of test samples = 6\n",
      "There is  <class 'list'>  of  <class 'pandas.core.frame.DataFrame'>\n",
      "The list was turned into <numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Set a percentage of test set fraction\n",
    "test_percent = 0.30 # 30%\n",
    "\n",
    "# Divide data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_percent, shuffle=True)\n",
    "len_train = len(X_train)\n",
    "len_test = len(X_test)\n",
    "\n",
    "print ('Number of train samples = {}\\nNumber of test samples = {}'.format(len_train, len_test))\n",
    "print ('There is ',type(X_train),' of ',type(X_train[0]))\n",
    "\n",
    "# Turn list(DataFrame) into numpy.ndarray with [len_train, NUMBER_TIMESTEPS, NUMBER_FEATURES]\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "print('The list was turned into <numpy.ndarray>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cur_batch_size):\n",
    "    model = models.Sequential()    \n",
    "    # an input layer that expects:\n",
    "    # 1 or more samples, NUMBER_TIMESTEPS time steps and NUMBER_FEATURES features.\n",
    "    \n",
    "    # 1st LSTM layer\n",
    "    model.add(layers.LSTM(256, return_sequences=True, input_shape=(NUMBER_TIMESTEPS, NUMBER_FEATURES)) )\n",
    "    \n",
    "    # 2nd LSTM layer\n",
    "    model.add(layers.LSTM(256, input_shape=(NUMBER_TIMESTEPS, NUMBER_FEATURES)) )\n",
    "    \n",
    "    # Hidden fully connected layers of the neural network\n",
    "    model.add(layers.Dense(512,activation='relu'))  \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(256,activation='relu'))    \n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(512,activation='relu'))\n",
    "    \n",
    "    # Classification layer of the neural network\n",
    "    model.add(layers.Dense(NUMBER_OUTPUTS, activation='softmax'))\n",
    "    \n",
    "    opt = Adam(learning_rate=0.002)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    # this shows the network structure \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 256)          470016    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,390,850\n",
      "Trainable params: 1,390,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating the model\n",
    "model = build_model(len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7067 - accuracy: 0.4286\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.6604 - accuracy: 0.5714\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6489 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7857\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3694 - accuracy: 0.9286\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.1132 - accuracy: 0.9286\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5441e-06 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 996us/step - loss: 1.1751e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x206a6fefa90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 4.7983 - accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of matches: 5\n",
      "Match rate: 0.83\n"
     ]
    }
   ],
   "source": [
    "matches = (y_pred == y_test)\n",
    "print('Total of matches: %d' % (matches.sum()))\n",
    "\n",
    "match_rate = matches.sum() / float(len(matches))\n",
    "print('Match rate: %.2f' % (match_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m, y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
